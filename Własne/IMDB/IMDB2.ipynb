{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from collections import Counter\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold,cross_val_score,GridSearchCV,train_test_split\n",
    "import warnings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "warnings.filterwarnings('ignore')\n",
    "import sklearn.metrics\n",
    "pd.set_option('max_columns', None)\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"~/Regr/IMDB/train.csv\")\n",
    "test = pd.read_csv(\"~/Regr/IMDB/test.csv\")\n",
    "dodat_train = pd.read_csv(\"~/Regr/IMDB/TrainAdditionalFeatures.csv\")\n",
    "dodat_test = pd.read_csv(\"~/Regr/IMDB/TestAdditionalFeatures.csv\")\n",
    "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
    "def text_to_dict(df):\n",
    "    for column in dict_columns:\n",
    "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df\n",
    "\n",
    "train = text_to_dict(train)\n",
    "test = text_to_dict(test)\n",
    "revenue = np.log1p(train['revenue'])\n",
    "y = revenue\n",
    "index = test['id']\n",
    "train_length = train.shape[0]\n",
    "train.drop(['revenue'],inplace=True,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\n",
    "train.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \n",
    "train.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\n",
    "train.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\n",
    "train.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \n",
    "train.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\n",
    "train.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\n",
    "train.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\n",
    "train.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\n",
    "train.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\n",
    "train.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\n",
    "train.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\n",
    "train.loc[train['id'] == 1007,'budget'] = 2              # Zyzzyx Road \n",
    "train.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\n",
    "train.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \n",
    "train.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \n",
    "train.loc[train['id'] == 1542,'budget'] = 1              # All at Once\n",
    "train.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\n",
    "train.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\n",
    "train.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\n",
    "train.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\n",
    "train.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\n",
    "train.loc[train['id'] == 1885,'budget'] = 12             # In the Cut\n",
    "train.loc[train['id'] == 2091,'budget'] = 10             # Deadfall\n",
    "train.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\n",
    "train.loc[train['id'] == 2491,'budget'] = 6              # Never Talk to Strangers\n",
    "train.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\n",
    "train.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\n",
    "train.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\n",
    "train.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\n",
    "train.loc[train['id'] == 335,'budget'] = 2 \n",
    "train.loc[train['id'] == 348,'budget'] = 12\n",
    "train.loc[train['id'] == 470,'budget'] = 13000000 \n",
    "train.loc[train['id'] == 513,'budget'] = 1100000\n",
    "train.loc[train['id'] == 640,'budget'] = 6 \n",
    "train.loc[train['id'] == 696,'budget'] = 1\n",
    "train.loc[train['id'] == 797,'budget'] = 8000000 \n",
    "train.loc[train['id'] == 850,'budget'] = 1500000\n",
    "train.loc[train['id'] == 1199,'budget'] = 5 \n",
    "train.loc[train['id'] == 1282,'budget'] = 9               # Death at a Funeral\n",
    "train.loc[train['id'] == 1347,'budget'] = 1\n",
    "train.loc[train['id'] == 1755,'budget'] = 2\n",
    "train.loc[train['id'] == 1801,'budget'] = 5\n",
    "train.loc[train['id'] == 1918,'budget'] = 592 \n",
    "train.loc[train['id'] == 2033,'budget'] = 4\n",
    "train.loc[train['id'] == 2118,'budget'] = 344 \n",
    "train.loc[train['id'] == 2252,'budget'] = 130\n",
    "train.loc[train['id'] == 2256,'budget'] = 1 \n",
    "train.loc[train['id'] == 2696,'budget'] = 10000000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Clean Data\n",
    "test.loc[test['id'] == 6733,'budget'] = 5000000\n",
    "test.loc[test['id'] == 3889,'budget'] = 15000000\n",
    "test.loc[test['id'] == 6683,'budget'] = 50000000\n",
    "test.loc[test['id'] == 5704,'budget'] = 4300000\n",
    "test.loc[test['id'] == 6109,'budget'] = 281756\n",
    "test.loc[test['id'] == 7242,'budget'] = 10000000\n",
    "test.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\n",
    "test.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\n",
    "test.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n",
    "test.loc[test['id'] == 3033,'budget'] = 250 \n",
    "test.loc[test['id'] == 3051,'budget'] = 50\n",
    "test.loc[test['id'] == 3084,'budget'] = 337\n",
    "test.loc[test['id'] == 3224,'budget'] = 4  \n",
    "test.loc[test['id'] == 3594,'budget'] = 25  \n",
    "test.loc[test['id'] == 3619,'budget'] = 500  \n",
    "test.loc[test['id'] == 3831,'budget'] = 3  \n",
    "test.loc[test['id'] == 3935,'budget'] = 500  \n",
    "test.loc[test['id'] == 4049,'budget'] = 995946 \n",
    "test.loc[test['id'] == 4424,'budget'] = 3  \n",
    "test.loc[test['id'] == 4460,'budget'] = 8  \n",
    "test.loc[test['id'] == 4555,'budget'] = 1200000 \n",
    "test.loc[test['id'] == 4624,'budget'] = 30 \n",
    "test.loc[test['id'] == 4645,'budget'] = 500 \n",
    "test.loc[test['id'] == 4709,'budget'] = 450 \n",
    "test.loc[test['id'] == 4839,'budget'] = 7\n",
    "test.loc[test['id'] == 3125,'budget'] = 25 \n",
    "test.loc[test['id'] == 3142,'budget'] = 1\n",
    "test.loc[test['id'] == 3201,'budget'] = 450\n",
    "test.loc[test['id'] == 3222,'budget'] = 6\n",
    "test.loc[test['id'] == 3545,'budget'] = 38\n",
    "test.loc[test['id'] == 3670,'budget'] = 18\n",
    "test.loc[test['id'] == 3792,'budget'] = 19\n",
    "test.loc[test['id'] == 3881,'budget'] = 7\n",
    "test.loc[test['id'] == 3969,'budget'] = 400\n",
    "test.loc[test['id'] == 4196,'budget'] = 6\n",
    "test.loc[test['id'] == 4221,'budget'] = 11\n",
    "test.loc[test['id'] == 4222,'budget'] = 500\n",
    "test.loc[test['id'] == 4285,'budget'] = 11\n",
    "test.loc[test['id'] == 4319,'budget'] = 1\n",
    "test.loc[test['id'] == 4639,'budget'] = 10\n",
    "test.loc[test['id'] == 4719,'budget'] = 45\n",
    "test.loc[test['id'] == 4822,'budget'] = 22\n",
    "test.loc[test['id'] == 4829,'budget'] = 20\n",
    "test.loc[test['id'] == 4969,'budget'] = 20\n",
    "test.loc[test['id'] == 5021,'budget'] = 40 \n",
    "test.loc[test['id'] == 5035,'budget'] = 1 \n",
    "test.loc[test['id'] == 5063,'budget'] = 14 \n",
    "test.loc[test['id'] == 5119,'budget'] = 2 \n",
    "test.loc[test['id'] == 5214,'budget'] = 30 \n",
    "test.loc[test['id'] == 5221,'budget'] = 50 \n",
    "test.loc[test['id'] == 4903,'budget'] = 15\n",
    "test.loc[test['id'] == 4983,'budget'] = 3\n",
    "test.loc[test['id'] == 5102,'budget'] = 28\n",
    "test.loc[test['id'] == 5217,'budget'] = 75\n",
    "test.loc[test['id'] == 5224,'budget'] = 3 \n",
    "test.loc[test['id'] == 5469,'budget'] = 20 \n",
    "test.loc[test['id'] == 5840,'budget'] = 1 \n",
    "test.loc[test['id'] == 5960,'budget'] = 30\n",
    "test.loc[test['id'] == 6506,'budget'] = 11 \n",
    "test.loc[test['id'] == 6553,'budget'] = 280\n",
    "test.loc[test['id'] == 6561,'budget'] = 7\n",
    "test.loc[test['id'] == 6582,'budget'] = 218\n",
    "test.loc[test['id'] == 6638,'budget'] = 5\n",
    "test.loc[test['id'] == 6749,'budget'] = 8 \n",
    "test.loc[test['id'] == 6759,'budget'] = 50 \n",
    "test.loc[test['id'] == 6856,'budget'] = 10\n",
    "test.loc[test['id'] == 6858,'budget'] =  100\n",
    "test.loc[test['id'] == 6876,'budget'] =  250\n",
    "test.loc[test['id'] == 6972,'budget'] = 1\n",
    "test.loc[test['id'] == 7079,'budget'] = 8000000\n",
    "test.loc[test['id'] == 7150,'budget'] = 118\n",
    "test.loc[test['id'] == 6506,'budget'] = 118\n",
    "test.loc[test['id'] == 7225,'budget'] = 6\n",
    "test.loc[test['id'] == 7231,'budget'] = 85\n",
    "test.loc[test['id'] == 5222,'budget'] = 5\n",
    "test.loc[test['id'] == 5322,'budget'] = 90\n",
    "test.loc[test['id'] == 5350,'budget'] = 70\n",
    "test.loc[test['id'] == 5378,'budget'] = 10\n",
    "test.loc[test['id'] == 5545,'budget'] = 80\n",
    "test.loc[test['id'] == 5810,'budget'] = 8\n",
    "test.loc[test['id'] == 5926,'budget'] = 300\n",
    "test.loc[test['id'] == 5927,'budget'] = 4\n",
    "test.loc[test['id'] == 5986,'budget'] = 1\n",
    "test.loc[test['id'] == 6053,'budget'] = 20\n",
    "test.loc[test['id'] == 6104,'budget'] = 1\n",
    "test.loc[test['id'] == 6130,'budget'] = 30\n",
    "test.loc[test['id'] == 6301,'budget'] = 150\n",
    "test.loc[test['id'] == 6276,'budget'] = 100\n",
    "test.loc[test['id'] == 6473,'budget'] = 100\n",
    "test.loc[test['id'] == 6842,'budget'] = 30\n",
    "\n",
    "power_six = train.id[train.budget > 1000][train.revenue < 100]\n",
    "\n",
    "for k in power_six :\n",
    "    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Kfold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-14d7944b4eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lgb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_feature_importance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Kfold' is not defined"
     ]
    }
   ],
   "source": [
    "cv = Kfold(n_splits = 10)\n",
    "def train_model(X, test, y, params=None, folds=cv, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    prediction = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if model_type == 'sklearn':\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.values[train_index], X.values[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, nthread = 4, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "                    verbose=1000, early_stopping_rounds=500)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=50000, evals=watchlist, early_stopping_rounds=500, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_squared_error(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train,dodat_train,how='left',on=['imdb_id'])\n",
    "test = pd.merge(test,dodat_test,how='left',on=['imdb_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train,test],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['has_collection'] = dataset['belongs_to_collection'].apply(lambda x: len(x) if x!={} else 0 )\n",
    "dataset['collection_name'] = dataset['belongs_to_collection'].apply(lambda x: x[0]['name'] if x!={} else 0)\n",
    "dataset.drop(['belongs_to_collection'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['log_budget'] = np.log1p(dataset['budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Genres = list(dataset['genres'].apply(lambda x: [i['name'] for i in x] if x!= [] else []).values)\n",
    "count = dict(Counter([i for j in Genres for i in j]).most_common(15))\n",
    "count = list(count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset['num_genres'] = dataset['genres'].apply(lambda x: len(x) if x!={} else 0 )\n",
    "\n",
    "dataset['all_genres'] = dataset['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x !={} else '')\n",
    "\n",
    "for i in count:\n",
    "   \n",
    "    dataset['genre_'+ i ] = dataset['all_genres'].apply(lambda x: 1 if i in x else 0)\n",
    "    \n",
    "dataset.drop(['genres'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset['companies_num'] = dataset['production_companies'].apply(lambda x: len(x) if x!={} else 0)\n",
    "\n",
    "\n",
    "dataset['all_companies'] = dataset['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x !={} else '')\n",
    "\n",
    "companies = list(dataset['production_companies'].apply(lambda x: [i['name'] for i in x] if x!= [] else []).values)\n",
    "comp = dict(Counter([i for j in companies for i in j]).most_common(15))\n",
    "comp = list(comp.keys())\n",
    "\n",
    "\n",
    "for i in comp:\n",
    "    \n",
    "    dataset['comp_'+ i ] = dataset['all_companies'].apply(lambda x: 1 if i in x else 0)\n",
    "    \n",
    "\n",
    "dataset.drop(['production_companies','all_companies'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset['countries_num'] = dataset['production_countries'].apply(lambda x: len(x) if x!={} else 0)\n",
    "\n",
    "\n",
    "dataset['all_countries'] = dataset['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x !={} else '')\n",
    "\n",
    "companies = list(dataset['production_countries'].apply(lambda x: [i['name'] for i in x] if x!= [] else []).values)\n",
    "comp = dict(Counter([i for j in companies for i in j]).most_common(15))\n",
    "comp = list(comp.keys())\n",
    "\n",
    "\n",
    "for i in comp:\n",
    "   \n",
    "    dataset['countr_'+ i ] = dataset['all_countries'].apply(lambda x: 1 if i in x else 0)\n",
    "    \n",
    "\n",
    "dataset.drop(['production_countries'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset['languages_num'] = dataset['spoken_languages'].apply(lambda x: len(x) if x!={} else 0)\n",
    "\n",
    "\n",
    "dataset['all_languages'] = dataset['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x !={} else '')\n",
    "\n",
    "companies = list(dataset['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x!= [] else []).values)\n",
    "comp = dict(Counter([i for j in companies for i in j]).most_common(15))\n",
    "comp = list(comp.keys())\n",
    "\n",
    "\n",
    "for i in comp:\n",
    "    dataset['lang_'+ i ] = dataset['spoken_languages'].apply(lambda x: 1 if i in x else 0)\n",
    "    \n",
    "\n",
    "dataset.drop(['spoken_languages','all_languages'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(dataset['Keywords'].apply(lambda x : [i['name'] for i in x] if x!={} else []).values)\n",
    "words = dict(Counter([i for j in words for i in j]).most_common(10))\n",
    "words = words.keys()\n",
    "\n",
    "for i in words:\n",
    "    dataset['word_'+ i ] = dataset['Keywords'].apply(lambda x: 1 if i in x else 0)\n",
    "    \n",
    "dataset.drop(['Keywords'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = dataset['cast'].apply(lambda x: [i['name'] for i in x])\n",
    "actors = Counter([i for j in actors for i in j]).most_common(15)\n",
    "actors = dict(actors).keys()\n",
    "\n",
    "for i in actors:\n",
    "    dataset['act_'+ i ] = dataset['cast'].apply(lambda x: 1 if i in x else 0)\n",
    "\n",
    "    \n",
    "dataset['cast_num'] = dataset['cast'].apply(lambda x: len(x) if x!={} else 0)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset['genders_0_cast'] = dataset['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n",
    "dataset['genders_1_cast'] = dataset['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n",
    "dataset['genders_2_cast'] = dataset['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n",
    "\n",
    "dataset.drop(['cast'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = train['crew'].apply(lambda x: [i['name'] for i in x])\n",
    "crew = Counter([i for j in crew for i in j]).most_common(15)\n",
    "crew = dict(crew).keys()\n",
    "for i in crew:\n",
    "    dataset['crew_'+ i ] = dataset['crew'].apply(lambda x: 1 if i in str(x) else 0)\n",
    "\n",
    "    \n",
    "\n",
    "dataset['crew_num'] = dataset['crew'].apply(lambda x: len(x) if x!={} else 0)    \n",
    "\n",
    "list_of_crew_jobs = list(dataset['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\n",
    "list_of_crew_departments = list(dataset['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\n",
    "\n",
    "\n",
    "top_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\n",
    "top_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\n",
    "\n",
    "\n",
    "    \n",
    "dataset['genders_0_crew'] = dataset['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n",
    "dataset['genders_1_crew'] = dataset['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n",
    "dataset['genders_2_crew'] = dataset['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n",
    "\n",
    "\n",
    "\n",
    "for j in top_crew_jobs:\n",
    "    dataset['jobs_' + j] = dataset['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\n",
    "for j in top_crew_departments:\n",
    "    dataset['departments_' + j] = dataset['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "dataset.drop(['crew'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['homepage'].fillna(0,inplace=True)\n",
    "dataset.loc[dataset['homepage']!=0,'homepage'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating(code):\n",
    "    url = f\"https://www.imdb.com/title/\"+ code\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    ans  = str(soup.find('span',itemprop='ratingValue'))\n",
    "    rating = re.findall(\"\\d+\\.\\d+\",ans)[0]\n",
    "    return float(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['release_month','release_day','release_year']]=dataset['release_date'].str.split('/',expand=True).replace(np.nan, -1).astype(int)\n",
    "\n",
    "# Some rows have 4 digits of year instead of 2, that's why I am applying (train['release_year'] < 100) this condition\n",
    "dataset.loc[ (dataset['release_year'] <= 19) & (dataset['release_year'] < 100), \"release_year\"] += 2000\n",
    "dataset.loc[ (dataset['release_year'] > 19)  & (dataset['release_year'] < 100), \"release_year\"] += 1900\n",
    "\n",
    "releaseDate = pd.to_datetime(dataset['release_date']) \n",
    "dataset['release_dayofweek'] = releaseDate.dt.dayofweek\n",
    "dataset['release_quarter'] = releaseDate.dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(df):\n",
    "    df['budget_to_popularity'] = df['budget'] / df['popularity']\n",
    "    df['budget_to_runtime'] = df['budget'] / df['runtime']\n",
    "    \n",
    "    # some features from https://www.kaggle.com/somang1418/happy-valentines-day-and-keep-kaggling-3\n",
    "    df['_budget_year_ratio'] = df['budget'] / (df['release_year'] * df['release_year'])\n",
    "    df['_releaseYear_popularity_ratio'] = df['release_year'] / df['popularity']\n",
    "    df['_releaseYear_popularity_ratio2'] = df['popularity'] / df['release_year']\n",
    "    \n",
    "    df['runtime_to_mean_year'] = df['runtime'] / df.groupby(\"release_year\")[\"runtime\"].transform('mean')\n",
    "    df['popularity_to_mean_year'] = df['popularity'] / df.groupby(\"release_year\")[\"popularity\"].transform('mean')\n",
    "    df['budget_to_mean_year'] = df['budget'] / df.groupby(\"release_year\")[\"budget\"].transform('mean')\n",
    "        \n",
    "    return df\n",
    "dataset = new_features(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Sep 17 11:55:06 2019\n",
      "Fold 1 started at Tue Sep 17 11:55:06 2019\n",
      "Fold 2 started at Tue Sep 17 11:55:07 2019\n",
      "Fold 3 started at Tue Sep 17 11:55:07 2019\n",
      "Fold 4 started at Tue Sep 17 11:55:08 2019\n",
      "Fold 5 started at Tue Sep 17 11:55:08 2019\n",
      "Fold 6 started at Tue Sep 17 11:55:09 2019\n",
      "Fold 7 started at Tue Sep 17 11:55:09 2019\n",
      "Fold 8 started at Tue Sep 17 11:55:09 2019\n",
      "Fold 9 started at Tue Sep 17 11:55:10 2019\n",
      "CV mean score: 3.0363, std: 0.1663.\n",
      "Fold 0 started at Tue Sep 17 11:55:11 2019\n",
      "Fold 1 started at Tue Sep 17 11:55:11 2019\n",
      "Fold 2 started at Tue Sep 17 11:55:12 2019\n",
      "Fold 3 started at Tue Sep 17 11:55:12 2019\n",
      "Fold 4 started at Tue Sep 17 11:55:13 2019\n",
      "Fold 5 started at Tue Sep 17 11:55:13 2019\n",
      "Fold 6 started at Tue Sep 17 11:55:14 2019\n",
      "Fold 7 started at Tue Sep 17 11:55:15 2019\n",
      "Fold 8 started at Tue Sep 17 11:55:15 2019\n",
      "Fold 9 started at Tue Sep 17 11:55:16 2019\n",
      "CV mean score: 2.9644, std: 0.1576.\n",
      "Fold 0 started at Tue Sep 17 11:55:19 2019\n",
      "Fold 1 started at Tue Sep 17 11:55:20 2019\n",
      "Fold 2 started at Tue Sep 17 11:55:21 2019\n",
      "Fold 3 started at Tue Sep 17 11:55:22 2019\n",
      "Fold 4 started at Tue Sep 17 11:55:23 2019\n",
      "Fold 5 started at Tue Sep 17 11:55:24 2019\n",
      "Fold 6 started at Tue Sep 17 11:55:25 2019\n",
      "Fold 7 started at Tue Sep 17 11:55:26 2019\n",
      "Fold 8 started at Tue Sep 17 11:55:27 2019\n",
      "Fold 9 started at Tue Sep 17 11:55:28 2019\n",
      "CV mean score: 2.9859, std: 0.1632.\n",
      "Fold 0 started at Tue Sep 17 11:55:29 2019\n",
      "Fold 1 started at Tue Sep 17 11:55:29 2019\n",
      "Fold 2 started at Tue Sep 17 11:55:30 2019\n",
      "Fold 3 started at Tue Sep 17 11:55:30 2019\n",
      "Fold 4 started at Tue Sep 17 11:55:30 2019\n",
      "Fold 5 started at Tue Sep 17 11:55:31 2019\n",
      "Fold 6 started at Tue Sep 17 11:55:31 2019\n",
      "Fold 7 started at Tue Sep 17 11:55:31 2019\n",
      "Fold 8 started at Tue Sep 17 11:55:32 2019\n",
      "Fold 9 started at Tue Sep 17 11:55:32 2019\n",
      "CV mean score: 3.0269, std: 0.1628.\n"
     ]
    }
   ],
   "source": [
    "train = dataset.iloc[:train_length]\n",
    "test = dataset.iloc[train_length:]\n",
    "train_texts = train[['title', 'tagline', 'overview', 'original_title']]\n",
    "test_texts = test[['title', 'tagline', 'overview', 'original_title']]\n",
    "for col in train_texts.columns:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                sublinear_tf=True,\n",
    "                analyzer='word',\n",
    "                token_pattern=r'\\w{1,}',\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=10\n",
    "    )\n",
    "    vectorizer.fit(list(train_texts[col].fillna('')) + list(test_texts[col].fillna('')))\n",
    "    train_col_text = vectorizer.transform(train_texts[col].fillna(''))\n",
    "    test_col_text = vectorizer.transform(test_texts[col].fillna(''))\n",
    "    model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=cv)\n",
    "    oof_text, prediction_text = train_model(train_col_text, test_col_text,y,folds=cv, params=None, model_type='sklearn', model=model)\n",
    "    \n",
    "    train[col + '_oof'] = oof_text\n",
    "    test[col + '_oof'] = prediction_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([train,test],sort=False)\n",
    "def top_cols_interaction(df):\n",
    "    df['budget_to_year'] = df['budget'] / df['release_year']\n",
    "    df['budget_to_mean_year_to_year'] = df['budget_to_mean_year'] / df['release_year']\n",
    "    df['popularity_to_mean_year_to_log_budget'] = df['popularity_to_mean_year'] / df['log_budget']\n",
    "    df['year_to_log_budget'] = df['release_year'] / df['log_budget']\n",
    "    df['budget_to_runtime_to_year'] = df['budget_to_runtime'] / df['release_year']\n",
    "    df['genders_1_cast_to_log_budget'] = df['genders_1_cast'] / df['log_budget']\n",
    "    df['all_genres_to_popularity_to_mean_year'] = df['num_genres'] / df['popularity_to_mean_year']\n",
    "    df['genders_2_crew_to_budget_to_mean_year'] = df['genders_2_crew'] / df['budget_to_mean_year']\n",
    "    df['overview_oof_to_genders_2_crew'] = df['overview_oof'] / df['genders_2_crew']\n",
    "    \n",
    "    return df\n",
    "dataset = top_cols_interaction(dataset)\n",
    "dataset.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status','all_countries'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['original_language', 'collection_name', 'all_genres']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(dataset[col].fillna('')))\n",
    "    dataset[col] = le.transform(dataset[col].fillna('').astype(str))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_texts = dataset[['title', 'tagline', 'overview', 'original_title']]\n",
    "\n",
    "for col in ['title', 'tagline', 'overview', 'original_title']:\n",
    "    dataset['len_' + col] = dataset[col].fillna('').apply(lambda x: len(str(x)))\n",
    "    dataset['words_' + col] = dataset[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n",
    "    dataset = dataset.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.replace([np.inf,-np.inf],np.nan)\n",
    "dataset.drop(['id','revenue'],inplace=True,axis=1)\n",
    "cols = dataset.columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "dataset = imputer.fit_transform(dataset)\n",
    "dataset = pd.DataFrame(dataset,columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:train_length]\n",
    "test = dataset.iloc[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Sep 17 14:21:53 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.22259\tvalid_1's rmse: 2.04806\n",
      "Early stopping, best iteration is:\n",
      "[643]\ttraining's rmse: 1.36461\tvalid_1's rmse: 2.04589\n",
      "Fold 1 started at Tue Sep 17 14:21:57 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.20021\tvalid_1's rmse: 2.12988\n",
      "Early stopping, best iteration is:\n",
      "[561]\ttraining's rmse: 1.39723\tvalid_1's rmse: 2.12603\n",
      "Fold 2 started at Tue Sep 17 14:22:00 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.23172\tvalid_1's rmse: 1.74647\n",
      "Early stopping, best iteration is:\n",
      "[912]\ttraining's rmse: 1.26664\tvalid_1's rmse: 1.74499\n",
      "Fold 3 started at Tue Sep 17 14:22:05 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.18689\tvalid_1's rmse: 2.06827\n",
      "[2000]\ttraining's rmse: 0.875852\tvalid_1's rmse: 2.05026\n",
      "Early stopping, best iteration is:\n",
      "[2426]\ttraining's rmse: 0.775635\tvalid_1's rmse: 2.04677\n",
      "Fold 4 started at Tue Sep 17 14:22:15 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.23422\tvalid_1's rmse: 1.81639\n",
      "Early stopping, best iteration is:\n",
      "[1493]\ttraining's rmse: 1.0614\tvalid_1's rmse: 1.81028\n",
      "Fold 5 started at Tue Sep 17 14:22:21 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.21644\tvalid_1's rmse: 1.68933\n",
      "Early stopping, best iteration is:\n",
      "[721]\ttraining's rmse: 1.33475\tvalid_1's rmse: 1.68135\n",
      "Fold 6 started at Tue Sep 17 14:22:26 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.24111\tvalid_1's rmse: 1.74726\n",
      "Early stopping, best iteration is:\n",
      "[925]\ttraining's rmse: 1.26997\tvalid_1's rmse: 1.74614\n",
      "Fold 7 started at Tue Sep 17 14:22:30 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.24438\tvalid_1's rmse: 1.70595\n",
      "Early stopping, best iteration is:\n",
      "[826]\ttraining's rmse: 1.31414\tvalid_1's rmse: 1.69914\n",
      "Fold 8 started at Tue Sep 17 14:22:34 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.22148\tvalid_1's rmse: 1.89523\n",
      "[2000]\ttraining's rmse: 0.915798\tvalid_1's rmse: 1.89281\n",
      "Early stopping, best iteration is:\n",
      "[2197]\ttraining's rmse: 0.868161\tvalid_1's rmse: 1.89237\n",
      "Fold 9 started at Tue Sep 17 14:22:43 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 1.22548\tvalid_1's rmse: 1.71509\n",
      "Early stopping, best iteration is:\n",
      "[522]\ttraining's rmse: 1.44267\tvalid_1's rmse: 1.71033\n",
      "CV mean score: 1.8503, std: 0.1579.\n"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 5,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "oof_lgb_1, prediction_lgb_1 = train_model(X, test, y, params=params, model_type='lgb', plot_feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Sep 17 14:21:16 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 0.638422\tvalid_1's rmse: 2.0893\n",
      "Early stopping, best iteration is:\n",
      "[585]\ttraining's rmse: 0.920883\tvalid_1's rmse: 2.07543\n",
      "Fold 1 started at Tue Sep 17 14:21:21 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 0.624376\tvalid_1's rmse: 2.12518\n",
      "Early stopping, best iteration is:\n",
      "[585]\ttraining's rmse: 0.920876\tvalid_1's rmse: 2.11447\n",
      "Fold 2 started at Tue Sep 17 14:21:26 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 0.640954\tvalid_1's rmse: 1.78871\n",
      "Early stopping, best iteration is:\n",
      "[501]\ttraining's rmse: 1.01065\tvalid_1's rmse: 1.75914\n",
      "Fold 3 started at Tue Sep 17 14:21:30 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[465]\ttraining's rmse: 1.03843\tvalid_1's rmse: 2.06656\n",
      "Fold 4 started at Tue Sep 17 14:21:33 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 0.627221\tvalid_1's rmse: 1.82055\n",
      "Early stopping, best iteration is:\n",
      "[571]\ttraining's rmse: 0.948307\tvalid_1's rmse: 1.79368\n",
      "Fold 5 started at Tue Sep 17 14:21:37 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[257]\ttraining's rmse: 1.32157\tvalid_1's rmse: 1.71189\n",
      "Fold 6 started at Tue Sep 17 14:21:40 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 0.639546\tvalid_1's rmse: 1.79906\n",
      "Early stopping, best iteration is:\n",
      "[559]\ttraining's rmse: 0.98064\tvalid_1's rmse: 1.76237\n",
      "Fold 7 started at Tue Sep 17 14:21:44 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[1000]\ttraining's rmse: 0.644685\tvalid_1's rmse: 1.73806\n",
      "Early stopping, best iteration is:\n",
      "[613]\ttraining's rmse: 0.926737\tvalid_1's rmse: 1.72656\n",
      "Fold 8 started at Tue Sep 17 14:21:48 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's rmse: 1.25171\tvalid_1's rmse: 1.86758\n",
      "Fold 9 started at Tue Sep 17 14:21:51 2019\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[220]\ttraining's rmse: 1.34954\tvalid_1's rmse: 1.70011\n",
      "CV mean score: 1.8578, std: 0.1560.\n"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 7,\n",
    "         'learning_rate': 0.02,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.7,\n",
    "         \"bagging_freq\": 5,\n",
    "         \"bagging_fraction\": 0.7,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "oof_lgb_2, prediction_lgb_2 = train_model(X, test, y, params=params, model_type='lgb', plot_feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "         'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 5,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "#model_lgb = lgb.LGBMRegressor(**params_lgb, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "#model_lgb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Sep 17 11:56:33 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[643]\ttraining's rmse: 1.36461\tvalid_1's rmse: 2.04589\n",
      "Fold 1 started at Tue Sep 17 11:56:36 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[561]\ttraining's rmse: 1.39723\tvalid_1's rmse: 2.12603\n",
      "Fold 2 started at Tue Sep 17 11:56:38 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's rmse: 1.23172\tvalid_1's rmse: 1.74647\n",
      "Early stopping, best iteration is:\n",
      "[912]\ttraining's rmse: 1.26664\tvalid_1's rmse: 1.74499\n",
      "Fold 3 started at Tue Sep 17 11:56:42 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's rmse: 1.18689\tvalid_1's rmse: 2.06827\n",
      "Early stopping, best iteration is:\n",
      "[1675]\ttraining's rmse: 0.964998\tvalid_1's rmse: 2.05107\n",
      "Fold 4 started at Tue Sep 17 11:56:48 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's rmse: 1.23422\tvalid_1's rmse: 1.81639\n",
      "Early stopping, best iteration is:\n",
      "[1493]\ttraining's rmse: 1.0614\tvalid_1's rmse: 1.81028\n",
      "Fold 5 started at Tue Sep 17 11:56:55 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[721]\ttraining's rmse: 1.33475\tvalid_1's rmse: 1.68135\n",
      "Fold 6 started at Tue Sep 17 11:56:59 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's rmse: 1.24111\tvalid_1's rmse: 1.74726\n",
      "Early stopping, best iteration is:\n",
      "[925]\ttraining's rmse: 1.26997\tvalid_1's rmse: 1.74614\n",
      "Fold 7 started at Tue Sep 17 11:57:05 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[578]\ttraining's rmse: 1.43073\tvalid_1's rmse: 1.69922\n",
      "Fold 8 started at Tue Sep 17 11:57:11 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's rmse: 1.22148\tvalid_1's rmse: 1.89523\n",
      "Early stopping, best iteration is:\n",
      "[988]\ttraining's rmse: 1.22666\tvalid_1's rmse: 1.89516\n",
      "Fold 9 started at Tue Sep 17 11:57:16 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[522]\ttraining's rmse: 1.44267\tvalid_1's rmse: 1.71033\n",
      "CV mean score: 1.8510, std: 0.1585.\n"
     ]
    }
   ],
   "source": [
    "oof_lgb, prediction_lgb = train_model(X, test, y, params=params_lgb, model_type='lgb', plot_feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Sep 17 14:02:29 2019\n",
      "[0]\ttrain-rmse:15.6104\tvalid_data-rmse:15.5801\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.28165\tvalid_data-rmse:2.02532\n",
      "[1000]\ttrain-rmse:0.979579\tvalid_data-rmse:1.99026\n",
      "[1500]\ttrain-rmse:0.776019\tvalid_data-rmse:1.98693\n",
      "[2000]\ttrain-rmse:0.62077\tvalid_data-rmse:1.98109\n",
      "[2500]\ttrain-rmse:0.499892\tvalid_data-rmse:1.97785\n",
      "[3000]\ttrain-rmse:0.4044\tvalid_data-rmse:1.97559\n",
      "[3500]\ttrain-rmse:0.327375\tvalid_data-rmse:1.97357\n",
      "[4000]\ttrain-rmse:0.26873\tvalid_data-rmse:1.97074\n",
      "[4500]\ttrain-rmse:0.219197\tvalid_data-rmse:1.97086\n",
      "[5000]\ttrain-rmse:0.179387\tvalid_data-rmse:1.9697\n",
      "[5500]\ttrain-rmse:0.147261\tvalid_data-rmse:1.96966\n",
      "Stopping. Best iteration:\n",
      "[5076]\ttrain-rmse:0.174159\tvalid_data-rmse:1.96918\n",
      "\n",
      "Fold 1 started at Tue Sep 17 14:03:22 2019\n",
      "[0]\ttrain-rmse:15.6125\tvalid_data-rmse:15.5609\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.27214\tvalid_data-rmse:2.11456\n",
      "[1000]\ttrain-rmse:0.98111\tvalid_data-rmse:2.10792\n",
      "Stopping. Best iteration:\n",
      "[763]\ttrain-rmse:1.09995\tvalid_data-rmse:2.10134\n",
      "\n",
      "Fold 2 started at Tue Sep 17 14:03:35 2019\n",
      "[0]\ttrain-rmse:15.5981\tvalid_data-rmse:15.693\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.30293\tvalid_data-rmse:1.76355\n",
      "[1000]\ttrain-rmse:0.995862\tvalid_data-rmse:1.73055\n",
      "Stopping. Best iteration:\n",
      "[931]\ttrain-rmse:1.03077\tvalid_data-rmse:1.7291\n",
      "\n",
      "Fold 3 started at Tue Sep 17 14:03:48 2019\n",
      "[0]\ttrain-rmse:15.6082\tvalid_data-rmse:15.6012\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.28563\tvalid_data-rmse:2.10935\n",
      "[1000]\ttrain-rmse:0.988198\tvalid_data-rmse:2.05796\n",
      "[1500]\ttrain-rmse:0.770276\tvalid_data-rmse:2.03982\n",
      "[2000]\ttrain-rmse:0.614963\tvalid_data-rmse:2.03066\n",
      "[2500]\ttrain-rmse:0.497323\tvalid_data-rmse:2.02835\n",
      "[3000]\ttrain-rmse:0.399895\tvalid_data-rmse:2.02623\n",
      "[3500]\ttrain-rmse:0.32232\tvalid_data-rmse:2.02534\n",
      "Stopping. Best iteration:\n",
      "[3117]\ttrain-rmse:0.380649\tvalid_data-rmse:2.0248\n",
      "\n",
      "Fold 4 started at Tue Sep 17 14:04:20 2019\n",
      "[0]\ttrain-rmse:15.6205\tvalid_data-rmse:15.4889\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.30194\tvalid_data-rmse:1.74642\n",
      "[1000]\ttrain-rmse:0.99314\tvalid_data-rmse:1.69847\n",
      "[1500]\ttrain-rmse:0.781803\tvalid_data-rmse:1.68975\n",
      "[2000]\ttrain-rmse:0.623085\tvalid_data-rmse:1.69247\n",
      "Stopping. Best iteration:\n",
      "[1652]\ttrain-rmse:0.72898\tvalid_data-rmse:1.68779\n",
      "\n",
      "Fold 5 started at Tue Sep 17 14:04:39 2019\n",
      "[0]\ttrain-rmse:15.6131\tvalid_data-rmse:15.5566\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.29782\tvalid_data-rmse:1.69415\n",
      "[1000]\ttrain-rmse:0.984813\tvalid_data-rmse:1.67196\n",
      "[1500]\ttrain-rmse:0.776179\tvalid_data-rmse:1.6678\n",
      "Stopping. Best iteration:\n",
      "[1455]\ttrain-rmse:0.79206\tvalid_data-rmse:1.66627\n",
      "\n",
      "Fold 6 started at Tue Sep 17 14:04:56 2019\n",
      "[0]\ttrain-rmse:15.6211\tvalid_data-rmse:15.4856\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.30205\tvalid_data-rmse:1.71448\n",
      "[1000]\ttrain-rmse:1.00032\tvalid_data-rmse:1.70101\n",
      "Stopping. Best iteration:\n",
      "[788]\ttrain-rmse:1.11153\tvalid_data-rmse:1.69749\n",
      "\n",
      "Fold 7 started at Tue Sep 17 14:05:07 2019\n",
      "[0]\ttrain-rmse:15.5956\tvalid_data-rmse:15.7163\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.30747\tvalid_data-rmse:1.72572\n",
      "[1000]\ttrain-rmse:0.999241\tvalid_data-rmse:1.70372\n",
      "[1500]\ttrain-rmse:0.792299\tvalid_data-rmse:1.70166\n",
      "Stopping. Best iteration:\n",
      "[1396]\ttrain-rmse:0.828954\tvalid_data-rmse:1.70017\n",
      "\n",
      "Fold 8 started at Tue Sep 17 14:05:23 2019\n",
      "[0]\ttrain-rmse:15.5943\tvalid_data-rmse:15.7268\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.2996\tvalid_data-rmse:1.84916\n",
      "[1000]\ttrain-rmse:0.992656\tvalid_data-rmse:1.82459\n",
      "[1500]\ttrain-rmse:0.78454\tvalid_data-rmse:1.8184\n",
      "[2000]\ttrain-rmse:0.624931\tvalid_data-rmse:1.81734\n",
      "Stopping. Best iteration:\n",
      "[1922]\ttrain-rmse:0.647421\tvalid_data-rmse:1.81598\n",
      "\n",
      "Fold 9 started at Tue Sep 17 14:05:44 2019\n",
      "[0]\ttrain-rmse:15.6011\tvalid_data-rmse:15.6654\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.30384\tvalid_data-rmse:1.71181\n",
      "[1000]\ttrain-rmse:1.00089\tvalid_data-rmse:1.68619\n",
      "[1500]\ttrain-rmse:0.786686\tvalid_data-rmse:1.67642\n",
      "[2000]\ttrain-rmse:0.626746\tvalid_data-rmse:1.67565\n",
      "[2500]\ttrain-rmse:0.504707\tvalid_data-rmse:1.67389\n",
      "[3000]\ttrain-rmse:0.406747\tvalid_data-rmse:1.67139\n",
      "[3500]\ttrain-rmse:0.328803\tvalid_data-rmse:1.67175\n",
      "Stopping. Best iteration:\n",
      "[3178]\ttrain-rmse:0.377725\tvalid_data-rmse:1.67013\n",
      "\n",
      "CV mean score: 1.8062, std: 0.1558.\n"
     ]
    }
   ],
   "source": [
    "params_xgb = {'eta': 0.01,\n",
    "              'objective': 'reg:linear',\n",
    "              'max_depth': 5,\n",
    "              'subsample': 0.8,\n",
    "              'colsample_bytree': 0.8,\n",
    "              'eval_metric': 'rmse',\n",
    "              'seed': 11,\n",
    "              'silent': True}\n",
    "oof_xgb, prediction_xgb = train_model(X, test, y, params=params_xgb,model_type='xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Sep 17 12:02:59 2019\n",
      "Fold 1 started at Tue Sep 17 12:08:48 2019\n",
      "Fold 2 started at Tue Sep 17 12:12:51 2019\n",
      "Fold 3 started at Tue Sep 17 12:18:51 2019\n",
      "Fold 4 started at Tue Sep 17 12:24:39 2019\n",
      "Fold 5 started at Tue Sep 17 12:29:05 2019\n",
      "Fold 6 started at Tue Sep 17 12:33:32 2019\n",
      "Fold 7 started at Tue Sep 17 12:37:42 2019\n",
      "Fold 8 started at Tue Sep 17 12:42:24 2019\n",
      "Fold 9 started at Tue Sep 17 12:45:56 2019\n",
      "CV mean score: 1.8873, std: 0.1370.\n"
     ]
    }
   ],
   "source": [
    "cat_params = {'learning_rate': 0.01,\n",
    "            'depth': 5,\n",
    "            'colsample_bylevel': 0.8,\n",
    "            'bagging_temperature': 0.2,\n",
    "            'random_seed': 11,\n",
    "            'loss_function': \"RMSE\",\n",
    "            'metric_period':None,\n",
    "             }\n",
    "\n",
    "oof_cat, prediction_cat = train_model(X, test, y, params=cat_params, model_type='cat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Sep 17 14:06:17 2019\n",
      "[0]\ttrain-rmse:15.6108\tvalid_data-rmse:15.5795\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.14303\tvalid_data-rmse:2.04294\n",
      "[1000]\ttrain-rmse:0.784799\tvalid_data-rmse:2.01056\n",
      "[1500]\ttrain-rmse:0.556774\tvalid_data-rmse:2.00353\n",
      "[2000]\ttrain-rmse:0.392827\tvalid_data-rmse:2.00131\n",
      "[2500]\ttrain-rmse:0.280837\tvalid_data-rmse:2.00013\n",
      "Stopping. Best iteration:\n",
      "[2191]\ttrain-rmse:0.345113\tvalid_data-rmse:1.99759\n",
      "\n",
      "Fold 1 started at Tue Sep 17 14:06:43 2019\n",
      "[0]\ttrain-rmse:15.6128\tvalid_data-rmse:15.5616\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.14036\tvalid_data-rmse:2.15441\n",
      "[1000]\ttrain-rmse:0.776105\tvalid_data-rmse:2.14773\n",
      "[1500]\ttrain-rmse:0.54466\tvalid_data-rmse:2.1457\n",
      "Stopping. Best iteration:\n",
      "[1188]\ttrain-rmse:0.680551\tvalid_data-rmse:2.14402\n",
      "\n",
      "Fold 2 started at Tue Sep 17 14:07:01 2019\n",
      "[0]\ttrain-rmse:15.5984\tvalid_data-rmse:15.6934\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.15776\tvalid_data-rmse:1.78489\n",
      "[1000]\ttrain-rmse:0.791099\tvalid_data-rmse:1.76698\n",
      "[1500]\ttrain-rmse:0.551823\tvalid_data-rmse:1.76975\n",
      "Stopping. Best iteration:\n",
      "[1052]\ttrain-rmse:0.761815\tvalid_data-rmse:1.76412\n",
      "\n",
      "Fold 3 started at Tue Sep 17 14:07:18 2019\n",
      "[0]\ttrain-rmse:15.6085\tvalid_data-rmse:15.6005\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.14769\tvalid_data-rmse:2.0847\n",
      "[1000]\ttrain-rmse:0.782688\tvalid_data-rmse:2.04215\n",
      "[1500]\ttrain-rmse:0.54726\tvalid_data-rmse:2.0274\n",
      "[2000]\ttrain-rmse:0.384901\tvalid_data-rmse:2.01917\n",
      "[2500]\ttrain-rmse:0.27323\tvalid_data-rmse:2.01829\n",
      "Stopping. Best iteration:\n",
      "[2463]\ttrain-rmse:0.280009\tvalid_data-rmse:2.01772\n",
      "\n",
      "Fold 4 started at Tue Sep 17 14:07:50 2019\n",
      "[0]\ttrain-rmse:15.6209\tvalid_data-rmse:15.4897\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.1652\tvalid_data-rmse:1.77063\n",
      "[1000]\ttrain-rmse:0.791694\tvalid_data-rmse:1.73006\n",
      "[1500]\ttrain-rmse:0.545884\tvalid_data-rmse:1.73011\n",
      "[2000]\ttrain-rmse:0.38046\tvalid_data-rmse:1.72869\n",
      "Stopping. Best iteration:\n",
      "[1834]\ttrain-rmse:0.428906\tvalid_data-rmse:1.7274\n",
      "\n",
      "Fold 5 started at Tue Sep 17 14:08:28 2019\n",
      "[0]\ttrain-rmse:15.6133\tvalid_data-rmse:15.5561\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.16452\tvalid_data-rmse:1.71826\n",
      "[1000]\ttrain-rmse:0.794014\tvalid_data-rmse:1.70773\n",
      "Stopping. Best iteration:\n",
      "[774]\ttrain-rmse:0.93799\tvalid_data-rmse:1.70334\n",
      "\n",
      "Fold 6 started at Tue Sep 17 14:08:45 2019\n",
      "[0]\ttrain-rmse:15.6214\tvalid_data-rmse:15.4855\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.16822\tvalid_data-rmse:1.72964\n",
      "[1000]\ttrain-rmse:0.80175\tvalid_data-rmse:1.7114\n",
      "[1500]\ttrain-rmse:0.565646\tvalid_data-rmse:1.70872\n",
      "Stopping. Best iteration:\n",
      "[1091]\ttrain-rmse:0.755767\tvalid_data-rmse:1.70741\n",
      "\n",
      "Fold 7 started at Tue Sep 17 14:09:18 2019\n",
      "[0]\ttrain-rmse:15.5956\tvalid_data-rmse:15.7158\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.17689\tvalid_data-rmse:1.72483\n",
      "[1000]\ttrain-rmse:0.806664\tvalid_data-rmse:1.72798\n",
      "Stopping. Best iteration:\n",
      "[617]\ttrain-rmse:1.06863\tvalid_data-rmse:1.71578\n",
      "\n",
      "Fold 8 started at Tue Sep 17 14:09:33 2019\n",
      "[0]\ttrain-rmse:15.5944\tvalid_data-rmse:15.7274\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.1696\tvalid_data-rmse:1.85763\n",
      "[1000]\ttrain-rmse:0.798174\tvalid_data-rmse:1.84164\n",
      "[1500]\ttrain-rmse:0.557925\tvalid_data-rmse:1.84177\n",
      "Stopping. Best iteration:\n",
      "[1296]\ttrain-rmse:0.644281\tvalid_data-rmse:1.83974\n",
      "\n",
      "Fold 9 started at Tue Sep 17 14:10:11 2019\n",
      "[0]\ttrain-rmse:15.6011\tvalid_data-rmse:15.6651\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 500 rounds.\n",
      "[500]\ttrain-rmse:1.17448\tvalid_data-rmse:1.72216\n",
      "[1000]\ttrain-rmse:0.802624\tvalid_data-rmse:1.69698\n",
      "[1500]\ttrain-rmse:0.56184\tvalid_data-rmse:1.68906\n",
      "[2000]\ttrain-rmse:0.396248\tvalid_data-rmse:1.6862\n",
      "Stopping. Best iteration:\n",
      "[1982]\ttrain-rmse:0.400799\tvalid_data-rmse:1.68592\n",
      "\n",
      "CV mean score: 1.8303, std: 0.1555.\n"
     ]
    }
   ],
   "source": [
    "params_xgb_1 = {'eta': 0.01,\n",
    "              'objective': 'reg:linear',\n",
    "              'max_depth': 6,\n",
    "              'subsample': 0.6,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'eval_metric': 'rmse',\n",
    "              'seed': 11,\n",
    "              'silent': True}\n",
    "oof_xgb_1, prediction_xgb_1 = train_model(X, test, y, params=params_xgb_1,model_type='xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = pd.Series(prediction_xgb).apply(lambda x: np.exp(x))\n",
    "xgb_1 = pd.Series(prediction_xgb_1).apply(lambda x: np.exp(x))\n",
    "cat = pd.Series(prediction_cat).apply(lambda x: np.exp(x))\n",
    "lgb = pd.Series(prediction_lgb).apply(lambda x: np.exp(x))\n",
    "lgb_1 = pd.Series(prediction_lgb_1).apply(lambda x: np.exp(x))\n",
    "lgb_2= pd.Series(prediction_lgb_2).apply(lambda x: np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pd.read_csv(\"~/Regr/IMDB/sample_submission.csv\")\n",
    "\n",
    "answer['revenue'] = (xgb_1*0.7)+(lgb_1+lgb_2+cat)*0.1\n",
    "answer.to_csv('proba_norm3_11.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
