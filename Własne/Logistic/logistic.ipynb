{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitwumprojekt1condaed2b3fa0ea9545aa93fe4c48967ebc97",
   "display_name": "Python 3.7.6 64-bit ('wum_projekt_1': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) \n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic():\n",
    "    '''Implementacja modelu regresji logistycznej trenowana poprzez użycie SGD z momentum '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        self.conf_matrix = None\n",
    "\n",
    "    \n",
    "    def sigmoid(self,X:np.ndarray) -> float:\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    " \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray,iterations=30,learning_rate=0.01,gamma=0.9,C=1.0) ->None:\n",
    "        '''Zdecydowałem się na użycie regularyzacji, aby poprawić wyniki algorytmu. Jej mocą steruje się poprzez odpowiednie ustawienie parametru C,\n",
    "            domyślnie jest on ustawiony na 1. Ponadto można ustawić ilość iteracji algorytmu, tempo ucznenia, a także moc momentum. '''\n",
    "        assert C !=0\n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]+1\n",
    "        X_b = np.c_[np.ones((m,1)),X]\n",
    "        velocity = 0 \n",
    "        self.theta = np.random.randn(n,1) \n",
    "        lamb = 1/C\n",
    "        for iteration in range(iterations):\n",
    "            for i in range(m):\n",
    "                random_index = np.random.randint(m)\n",
    "                xi = X_b[random_index:random_index+1]\n",
    "                yi = y[random_index:random_index+1]\n",
    "                sigm = self.sigmoid(xi @ self.theta)\n",
    "                gradient = xi.T.dot (sigm - yi) + (lamb/m)*(self.theta)\n",
    "                velocity = gamma * velocity\n",
    "                speed = learning_rate*gradient +velocity\n",
    "                self.theta = self.theta - speed\n",
    "        return\n",
    "    \n",
    "    def coef(self) -> np.ndarray:\n",
    "        '''Funkcja zwracająca parametry wytrenowane przez model '''\n",
    "        return self.theta\n",
    "    \n",
    "    def validate(self,X: np.ndarray,y:np.ndarray,cv=5) -> str:\n",
    "        '''Funkcja wykonująca kroswalidacje poprzez podzielenie zbioru na cv podzbiorów, a nastepnie iteracyjne tranowanie modelu na cv-1 częsciach, a \n",
    "            następnie sprawdzanie wyników na pozostałym zbiorze. W efekcie mamy cv wyników które są uśredniane i zwracane przez funkcje '''\n",
    "        size = X.shape[0]\n",
    "        scores = []\n",
    "        perm = np.random.permutation(size)\n",
    "        X_b = X[perm]\n",
    "        y_b = y[perm]\n",
    "        X_b = np.array(np.array_split(X_b,cv))\n",
    "        y_b = np.array(np.array_split(y_b,cv))\n",
    "        mask = np.ones(cv,dtype=bool) #Pomocnicza tablica do wybierania odpowiednich podtablic\n",
    "        for i in range(cv):\n",
    "            mask[i] = 0\n",
    "            X_val = X_b[mask==0][0]\n",
    "            y_val = y_b[mask==0][0]\n",
    "            res = X_b.shape[0] -y_val.shape[0]\n",
    "            X_fit = X_b[mask].reshape(res,(28*28))\n",
    "            y_fit = y_b[mask].reshape(res,)\n",
    "            self.fit(X_fit,y_fit)\n",
    "            pred = self.predict(X_val)\n",
    "            score =  self.evaluate(y_val,pred)\n",
    "            scores.append(score)\n",
    "            mask[i] = 1\n",
    "        return f'Mean = {np.mean(scores)}, Std = {np.std(scores)}'\n",
    "            \n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        '''Funckja do przewidywania na podstawie wcześniej wytrenowanych parametrów'''\n",
    "        m = X.shape[0]\n",
    "        X_b = np.c_[np.ones((m,1)),X]\n",
    "        return np.round(self.sigmoid(X_b @ self.theta))\n",
    "    \n",
    "\n",
    "    def evaluate(self,y_true: np.ndarray, y_pred: np.ndarray) ->float:\n",
    "        '''Funckja mierząca accuracy '''\n",
    "        assert y_true.shape[0] == y_pred.shape[0]\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i]==1 and y_true[i] == y_pred[i]:\n",
    "                TP +=1\n",
    "            elif y_true[i]==0 and y_true[i] == y_pred[i]:\n",
    "                TN +=1\n",
    "            elif  y_true[i] > y_pred[i]:\n",
    "                FN+=1\n",
    "            elif y_pred[i]>y_true[i]:\n",
    "                FP +=1\n",
    "        accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "        self.conf_matrix = np.array([[TP,FP],[FN,TN]])\n",
    "        return accuracy\n",
    "\n",
    "    def confusion(self) ->np.ndarray:\n",
    "        '''Funkcja zwracająca macierz pomyłek '''\n",
    "        return self.conf_matrix\n",
    "        \n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMNIST( prefix, folder ):\n",
    "    '''Funkcja do ładowania zbiou danych z pliku ubyte '''\n",
    "    intType = np.dtype( 'int32' ).newbyteorder( '>' )\n",
    "    nMetaDataBytes = 4 * intType.itemsize\n",
    "\n",
    "    data = np.fromfile( folder + \"/\" + prefix + '-images-idx3-ubyte', dtype = 'ubyte' )\n",
    "    magicBytes, nImages, width, height = np.frombuffer( data[:nMetaDataBytes].tobytes(), intType )\n",
    "    data = data[nMetaDataBytes:].astype( dtype = 'float32' ).reshape( [ nImages, width, height ] )\n",
    "\n",
    "    labels = np.fromfile( folder + \"/\" + prefix + '-labels-idx1-ubyte',\n",
    "                          dtype = 'ubyte' )[2 * intType.itemsize:]\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def delete_elems(X,y):\n",
    "    '''Usunięcie elementów zawierających 0 bądź 1, zgodnie z poleceniem'''\n",
    "    index = np.argwhere(np.logical_or(y==0,y==1))\n",
    "    X = np.delete(X,index,axis=0)\n",
    "    y= np.delete(y,index)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def change_labels(y):\n",
    "    '''Odpowiednie ustawienie labeli, tak że 1 odpowiada liczbom pierwszym, a 0 liczbom złożonym '''\n",
    "    index_prime = np.argwhere(np.logical_or.reduce((y==2,y==3,y==5,y==7)))\n",
    "    index_composite = np.argwhere(np.logical_or.reduce((y==4,y==6,y==8,y==9)))\n",
    "    y[index_prime] = 1\n",
    "    y[index_composite] = 0 \n",
    "    return y\n",
    "\n",
    "def resize(X):\n",
    "    '''Przeskalowanie zbioru danych, do przedziału [0,1] '''\n",
    "    n = X.shape[0]\n",
    "    X = X.reshape(n,28*28)\n",
    "    X = X/255\n",
    "    return X\n",
    "\n",
    "def shuffle(X,y):\n",
    "    '''Funkcja do losowego przemieszania zbioru '''\n",
    "    perm= np.random.permutation(X.shape[0])\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    return X,y\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    '''Funckja służąca jako wraper, przygotowujący zbiór danych do odpalenia na nim modelu '''\n",
    "    X_train, y_train = loadMNIST( \"train\", \"./Allegro_data\")\n",
    "    X_test, y_test = loadMNIST( \"t10k\", \"./Allegro_data\" )\n",
    "    X_train,y_train = delete_elems(X_train,y_train)\n",
    "    X_test,y_test = delete_elems(X_test,y_test)\n",
    "    y_train = change_labels(y_train)\n",
    "    y_test = change_labels(y_test)\n",
    "    X_train = resize(X_train)\n",
    "    X_test = resize(X_test)\n",
    "    X_train,y_train = shuffle(X_train,y_train)\n",
    "    X_test,y_test = shuffle(X_test,y_test)\n",
    "    return X_train,X_test,y_train,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.91857958148383"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = prepare()\n",
    "logistic = Logistic()\n",
    "logistic.fit(X_train,y_train)\n",
    "pred = logistic.predict(X_test)\n",
    "score =logistic.evaluate(y_test,pred)\n",
    "score\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uważam iż udało mi się osiągnąc dobre wyniki. Accuracy na zbiorze testowym na poziomie 91-92% jest wynikiem porównywalnym z wynikiem, który osiągnęła implementacja tego samego algorytmu w bibliotece Scikit-Learn. Mogę stwierdzić iż regularyzacja, oraz użycie momentum znacząco poprawiło działanie algorytmu. Obie metody poprawiły accuracy, a także pozwoliły ustabilizować wyniki. Jeśli chodzi o analizę błędów popełnianych przez algorytm, to jest to dość utrudnione ze wzgłędu na sformułwanie problemu dzielące zbiór na liczby pierwsze i złożone, przez co, nie można zdecydowanie stwierdzić na jakich cyfrach algorytm myli się najczęsciej. Aby osiągnąć lepsze accuracy należałoby użyć algorytmu zdolnego analizować bardziej skomplikowane wzorce, jak na przykład SVM."
   ]
  }
 ]
}